\section{Testing}
\subsection{Frontend User Testing}
This section covers the informal user testing executed for front end usability and subsequent changes to the user interface. As suggested in our meetings we have carried out some informal testing on the target audience to get advice and feedback on our front end implementation. 

\subsection{Testing process}
As this as informal testing we recruited some volunteers from our target demographic i.e. CS students with little to no knowledge of time series analysis. With regards to the process we set the application to its default starting mode and allowed the user to navigate the application independently so that we could observe the application's level of intuitiveness. As the user navigates the site one of our team members monitored and made notes on comments they made as well as features that were overlooked. Once completed the user was asked for any further comments and suggested improvements in order to document their full experience.

\subsection{Results}
Some comments that were brought up during testing including the action taken to mitigate this.
\begin{enumerate}
\item 'Toolbar not obvious what to do even with instructions'. As a result, clearer steps were added in starting instructions that included references to the location of the settings i.e. in the top bar.
\item 'Signal configuration not obvious how to start'. In response to this, instructions were added underneath form heading to prompt user to select a signal type.
\item 'Even with notification on bookmark it is not completely clear what the use of it is'. In further development more explanation was added to the bookmark popover to indicate what the user could do with this URL and made note to emphasise the bookmark feature in the user manual as it is clear that its there but not what its used for.
\item 'Could have header to identify or brand application'. A clear header with the application name was added to the UI to give the user instant indication of the application use.
\item 'Signal parameters could have more explanation'. A modal with signal data indications was added for each type of signal to make it more clear for users. 
\item 'Confused about the combination bar'. As well as making it clear in the user guide what this is used for a tool tip was also added to warn users about combining signals other than sinusoids and trends using the product method. 
\end{enumerate}
\subsubsection{Frontend Unit Testing}

\subsection{JavaScript Automated Testing}
This section details the automated test suites developed for the JavaScript portion of the application. 

If elected, these tests can be ran by any end-user with access to the source-code by entering the following command in a terminal launched from the directory: {\bf npm test}. This command runs all the test suites that Jest can find for the project and displays how many were successful and any errors or warnings that occur. 

\subsubsection{FileIO.js}
The following functionality has been tested for the File Uploading functionality of the application. 

\begin{enumerate}
    \item Verification of file prefix: This test checks that the {\bf fileTypeIsSupported(fileName)} method works as expected. The first test supplies a valid String filename, ending with a .csv prefix, and expects that the method will return 'true'.
    
    The next test supplies the same method with an invalid file name, {\it someInvalidFile.{\bf json}} and expects the method to return 'false'. This is because the file extension is not supported, hence we don't want it to return true for an unknown/unsupported file type.
    \item Verification of csv input to JavaScript Array: The first test creates a valid String of example csv test data (emulating real behaviour, as the {\it FileReader} object of {\bf parseFile()} method reads the csv as text.) A variable containing the expected output is then created below, but this time as a JavaScript array, and not encoded as a String object. The test then expects that, upon calling the {\bf csvToArray(csvData)} method with the supplied, valid csv string data, the method will return an object matching in structure to the expectedOutput variable created earlier.
    
    The following test then verifies that, upon being supplied with invalid, non-numerical csv data, an exception is thrown with the message {\bf Non-numerical data discovered in csv file.}
    
    Finally, the last test in this test suite verifies that csv data containing exponential values (formatted with e's and -'s) is successfully read and parsed by the class. Identical to the original test for parsing csv data, an String representation of some example csv test data is created along with an expected output as a variable containing a JavaScript array. The test calls the {\bf csvToArray(csvData)} method and expects to receive an object with a structure matching the expectedOutput variable created before.
\end{enumerate}

That concludes the automated testing for the FileIO module. You'll notice that there is no testing for 'reasonable' or 'sensible' data being uploaded to the application; the user should be able to experiment with the application however they see fit, and there should be as few guard rails as possible to prevent their exploration of these algorithms. 

\subsubsection{sessionStorage.js}
This section was a little trickier to test owing to the fact that the test requires a Browser's sessionStorage functionality in order to function correctly. As this test is running offline, absent a web browser, the sessionStorage functionality was initially very finicky and tricky to emulate/mock. Eventually a helpful library was discovered to emulate the sessionStorage functionality that integrated with Jest seamlessly; {\bf mock-local-storage}. As the name implies, this library mocks the local (\& and session) storage functionality of a web browser in the Jest framework. After the introduction of this library, testing was straightforward. 

Referenced within the tests of this helper class are a couple of constants required solely for testing: {\bf testKeyForSessionStorage} and {\bf validStringValue}. Both of these are relatively straightforward to comprehend: the first is the key which will be used to store the value of the object in session storage, the latter being the value to be stored.

The following is a record of how the functionality of this class was tested:
\begin{enumerate}
    \item Saving string to sessionStorage should succeed: As the name implies, this test verifies that upon saving an value to sessionStorage with this helper class with a given key, the data is, infact, stored in the browser's sessionStorage. 
    
    \item Saving JSON Stringified object should succeed and return valid string: This test ensures that an example JavaScript array, preserved and converted into a JSON String, can firstly be stored in sessionStorage, and secondly can then be retrieved without any corruption to the format of the JSON String. 
    
    \item Removing string saved in sessionStorage should succeed: This test removes the value associated with the given key in SessionStorage and verifies that upon requesting the value of the key afterwards, without rewriting a new value to the key, a {\it null} is returned. 
    
    \item Getting data from nonexistant key should return nothing: This test verifies that upon attempting to retrieve a value for a key which does not exist in sessionStorage (emulating behaviour of mis-typed parameter in code), the behaviour is safely handled and only a {\it null} reference is returned.
\end{enumerate}

As a result of our unit testing the majority of the JavaScript code (Excluding configSignal.js and bookmark.js) has been auto-tested as demonstrated in our coverage reports. These reports also covered external imported modules used for the 'fileIO' module and therefore the overall coverage may be an underestimation for our developed code

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/coveragereports.png}
    \caption{\label{fig:Coverage Report}Coverage Report}
\end{figure}

\subsection{JavaScript Manual Testing}
The frontend modules including 'bookmark.js', 'configSignal.js', 'graphs.js', 'infoPanel.js', 'reset.js' and 'uploadSignal.js' have been unit tested using Jest and the JSDOM plugin to some capacity though with time constraints and difficulties with mocking certain events and window objects 'bookmark.js' and 'config.js' do not have full function coverage in automated testing though they have all been manually tested through the common use cases which is outlined below.

\subsubsection{bookmark.js}
\begin{enumerate}
\item bookmarkToClipboard: This was checked through simply clicking on the bookmark button and checking if the current application URL was saved.
\item addParam, addSignalParam, removeSignalParam, editSignalParam: The tests for these involved checking the correct URL parameters were produced in the address bar of the browser. Additional comparison checks for ensuring the the URL parameter data is consistently passed to sessionStorage properly was also conducted. 
\item clearSignalParam, clearURLParam: These tests involved checking the URL again for the correct parameter clearance with test data.
\item paramsToObj: This takes the URL string and creates an object so in order to test this the URL was compared to the result of this function for any inconsistencies. 
\end{enumerate}

\subsubsection{configSignal.js}
\begin{enumerate}
\item displayDeleteBtn, displayAddBtn, displayEditBtn: These tests involved going through the common use cases of add, delete and editing signals to make sure the buttons appeared at the correct stages.
\item changeFormTemplate: This test was simply going through the different options of the select input on the signal configuration form and checking all the inputs are consistent with the signals schema that has been defined.
\item populateSettingsForm: This involved testing the edit functionality of the signals form as 
\item getEditSignalData: This was a intermediate function only used to edit the id of the signal data so it was simply tested by checking that the resultant signal data had a signal id that is consistent with the selected signal.
\item addSignalChip: This check was done visually as it creates a button to represent all the different signals, the page was also inspected to ensure the element was assigned the correct id.
\item getSignalData: This returns all the data from the signal form so test data was entered and checked for consistency through logs when the function was called.
\item addSignal: This function calls the addSignalChip function and increments the signal count as well as updating the session storage. This was tested through checking the signal count at different stages of adding signals and monitoring the session storage for changes.
\item showConfigureTab: This dynamically adds the elements of configure panel to the DOM using a template. This was tested visually through clicking on the tab button as well as checking the functionality of the different elements.
\end{enumerate}

\subsection{Python Manual Testing}
Text goes here...

\section{Conclusion}
This concludes the test report for the application. As has been demonstrated above, a robust testing strategy has been utilized to ensure individual components were as expected. Suitable, reasonable edge cases have been chosen to test with, as has realistic/expected behaviour and the combination of the two has lead to a robust final product.

With more foresight, and a few less hiccups throughout development (specifically little niggles with PyoDide and SessionStorage), our Integration testing strategy would've relied less on manual testing by developers and could've been automated by either Jest or a python testing framework to further ensure end-to-end robustness of the application. It also would've been more desirable to identify more browsers/browser versions which support our application, however with many browsers electing to make auto-updating an automatic, turn-off feature (i.e. Google Chrome) it was believed that most users will either be using a browser, or have access to one, which supports the application.